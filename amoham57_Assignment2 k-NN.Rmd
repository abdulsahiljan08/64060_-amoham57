---
title: "amoham57_Assignment 2"
author: "Abdul Sahil Mohammed"
date: "2024-02-24"
output: html_document
---
##Executive Summary 
#Universal Bank is a burgeoning financial institution experiencing rapid growth in customer acquisition. While the majority of its clientele consists of liability customers. The bank is keen on expanding its loan business by focusing on converting its liability customers into personal loan clients. A successful campaign targeting liability customers last year yielded a conversion rate exceeding 9%. The objective is to employ the k-NN algorithm to predict the likelihood of new customers accepting loan offers. This predictive analysis will inform the development of more effective and targeted marketing campaigns for the bank. The UniversalBank.csv file contains data on 5000 customers, including demographic information, banking relationships, and their response to the last personal loan campaign. Out of these, only 480 customers (9.6%) accepted the personal loan offer. The data will be split into 60% training and 40% validation sets.
```{r}
getwd()

### Import the data and do the cleaning 

 #load pacakges “class”,“caret”
library(class)
library(caret)

```
```{r}
library(readxl)
UB.Data <- read.csv("C:/Users/abdul/Documents/UniversalBank.csv")
dim(UB.Data)
```
```{r}
head(UB.Data)
```
```{r}
tail(UB.Data)
```
```{r}
## Transpose the dataframe
t(t(names(UB.Data))) 
```
```{r}
getwd()
```
```{r}
Real_Data <- UB.Data[,-c(1,5)]
dim(Real_Data)
```
##now, split the Data into two division. Training 60% and Validation 40%. also, Transform categorical variable into dummy variables

#changing the education attribute’s int value to char
```{r}
Real_Data$Education <- as.factor(Real_Data$Education)
```

#constructing the dummy variables for the attribute “education”
```{r}
dums<- dummyVars(~.,data=Real_Data)
Real_Data <- as.data.frame(predict(dums,Real_Data))
```

#now set seed and divide the data into 60% training and 40% validation to run the function 
```{r}
set.seed(1)
train.set_data <- sample(row.names(Real_Data), 0.6*dim(Real_Data)[1])
valid.set_data <- setdiff(row.names(Real_Data),train.set_data)
train <- Real_Data[train.set_data,]
valid <- Real_Data[valid.set_data,]
t(t(names(train)))
```
```{r}
summary(train)
```
```{r}
cat("The size of the training dataset is:",nrow(train))
```
```{r}
summary(valid)
```
```{r}
cat("The size of the validation dataset is:",nrow(valid))
```
##normalize the data 
```{r}
train.set_norm <- train[,-10]
valid.set_norm <- valid[,-10]
norm <- preProcess(train[,-10],method=c("center","scale"))
5
```
```{r}
train.set_norm <- predict(norm,train[,-10])
valid.set_norm <- predict(norm,valid[,-10])
```

#1.Age = 40, Experience = 10, Income = 84, Family = 2, CCAvg = 2, Education_1 = 0, Education_2 =1, Education_3 = 0, Mortgage = 0, Securities Account = 0, CD Account = 0, Online = 1, andCredit Card = 1. Perform a k-NN classification with all predictors except ID and ZIP codeusing k = 1. Generating the Data
```{r}
Newdata<- data.frame(
Age = 40,
Experience = 10,
Income = 84,
Family = 2,
CCAvg = 2,
Education.1 = 0,
Education.2 = 1,
Education.3 = 0,
Mortgage = 0,
Securities.Account = 0,
CD.Account = 0,
Online = 1,
CreditCard = 1
)

# Normalize the new customer dataset
customer_set.norm <- predict(norm, Newdata)
```
## Performing kNN classification
```{r}
prediction <- class::knn(train = train.set_norm,
test = customer_set.norm,
cl = train$Personal.Loan, k = 1)
prediction
```
##2.What is a choice of k that balances between overfitting and ignoring the predictor information?
```{r}
# Calculate the accuracy for each value of k
# Set the range of k values to consider
Accuracy <- data.frame(k = seq(1, 15, 1), overallaccuracy = rep(0, 15))
for(i in 1:15) {
kn <- class::knn(train = train.set_norm,
test = valid.set_norm,
cl = train$Personal.Loan, k = i)
Accuracy[i, 2] <- confusionMatrix(kn,
as.factor(valid$Personal.Loan),positive = "1")$overall[1]
}
which(Accuracy[,2] == max(Accuracy[,2]))
```
```{r}
Accuracy
```

The optimal performer within the range of k values from 1 to 15 is 3. This particular k value demonstrates the highest accuracy as it effectively balances between overfitting and underfitting predictions.
```{r}
plot(Accuracy$k,Accuracy$overallaccuracy)
```
validation Data using K.
##3.confusion matrix
```{r}
Prediction <- class::knn(train = train.set_norm,
test = valid.set_norm,
cl = train$Personal.Loan, k=3)
confusionMatrix(Prediction,as.factor(valid$Personal.Loan))
```
##4.Consider the following customer: Age = 40, Experience = 10, Income = 84,Family = 2, CCAvg = 2, Education_1 = 0, Education_2 = 1, Education_3 = 0,Mortgage = 0, Securities Account = 0, CD Account = 0, Online = 1 and CreditCard = 1. Classify the customer using the best k.

#Create identical column names in a dataframe for client 2.

```{r}
Data_2 <- data.frame(
Age = 40,
Experience = 10,
Income = 84,
Family = 2,
CCAvg = 2,
Education.1 = 0,
Education.2 = 1,
Education.3 = 0,
Mortgage = 0,
Securities.Account = 0,
CD.Account = 0,
Online = 1,
CreditCard = 1)

#Normalizing the 2nd client dataset
Data_2_norm <- predict(norm , Data_2)
```

##5.Repeat the process by dividing the data into three parts: 50%, 30%, and 20%.With the k selected above, use the k-NN technique. Contrast the confusion matrix of the test set alongside the confusion matrix of the training and validation sets. Explain the distinctions and why they exist.

```{r}
set.seed(500)
Train.set_Index <- sample(row.names(Real_Data), .5*dim(Real_Data)[1])
#create train index
9
```
```{r}
#create the validation index
Valid_Index <- sample(setdiff(row.names(Real_Data),Train.set_Index),.3*dim(Real_Data)[1])
Test_Index =setdiff(row.names(Real_Data),union(Train.set_Index,Valid_Index))#create test index
train.df <- Real_Data[Train.set_Index,]
cat("The size of the new training dataset is:", nrow(train.df))
```
```{r}
valid.df <- Real_Data[Valid_Index, ]
cat("size of new validation dataset is:", nrow(valid.df))
```
```{r}
test.df <- Real_Data[Test_Index, ]
cat("size of new test dataset is:", nrow(test.df))
```
Normalizing the following data
```{r}
normvalues <- preProcess(train.df[, -10], method=c("center", "scale"))
train.df.norm <- predict(norm, train.df[, -10])
valid.df.norm <- predict(norm, valid.df[, -10])
test.df.norm <- predict(norm ,test.df[,-10])
```

##
Execute kNN and generate confusion matrices for the training, testing, and validation datasets.
```{r}
predict_data <- class::knn(train = train.df.norm,
test = test.df.norm,
cl = train.df$Personal.Loan, k=3)
confusionMatrix(predict_data,as.factor(test.df$Personal.Loan))
```
```{r}
predict_data2 <- class::knn(train = train.df.norm,
test = valid.df.norm,
cl = train.df$Personal.Loan, k=3)
confusionMatrix(predict_data2,as.factor(valid.df$Personal.Loan))
```
```{r}
predict_data3 <- class::knn(train = train.df.norm,
test = train.df.norm,
cl = train.df$Personal.Loan, k=3)
confusionMatrix(predict_data3,as.factor(train.df$Personal.Loan))
```

